# 飞书文档图片导入机制深度分析

## 一、整体架构

PandaWiki 采用**微服务架构**处理飞书文档导入，涉及三个核心服务：

```
┌─────────────────┐      ┌──────────────────┐      ┌─────────────────┐
│                 │      │                  │      │                 │
│  PandaWiki API  │◄────►│  Crawler Service │◄────►│  Feishu API     │
│  (Go Backend)   │      │  (Python/Go)     │      │  (飞书开放平台)  │
│                 │      │                  │      │                 │
└─────────────────┘      └──────────────────┘      └─────────────────┘
        │                         │
        │                         │
        ▼                         ▼
┌─────────────────┐      ┌──────────────────┐
│                 │      │                  │
│  MinIO/S3       │      │  Message Queue   │
│  (对象存储)      │      │  (NATS)          │
│                 │      │                  │
└─────────────────┘      └──────────────────┘
```

### 服务职责

| 服务 | 地址 | 职责 |
|------|------|------|
| **PandaWiki API** | `http://panda-wiki-api:8000` | 主业务逻辑、用户请求处理 |
| **Crawler Service** | `http://panda-wiki-crawler:8080` | 文档抓取、格式转换、图片下载 |
| **MinIO/S3** | 配置的对象存储地址 | 图片和文件存储 |
| **NATS** | 消息队列 | 异步任务通知 |

---

## 二、飞书文档导入流程

### 2.1 完整流程图

```
用户发起导入请求
    ↓
[1] API 接收请求
    POST /api/v1/crawler/parse
    {
      "crawler_source": "feishu",
      "feishu_setting": {
        "app_id": "xxx",
        "app_secret": "xxx",
        "user_access_token": "xxx"
      }
    }
    ↓
[2] API 调用 Crawler Service
    GET http://panda-wiki-crawler:8080/api/docs/feishu/list
    参数: uuid, app_id, app_secret, access_token
    ↓
[3] Crawler Service 调用飞书 API
    获取文档列表和元数据
    ↓
[4] 返回文档列表给前端
    用户选择要导入的文档
    ↓
[5] 用户点击导入特定文档
    ↓
[6] API 调用 Crawler Service 导出文档
    POST http://panda-wiki-crawler:8080/api/docs/feishu/export
    {
      "uuid": "xxx",
      "doc_id": "xxx",
      "file_type": "docx",
      "uploader": {
        "type": 1,  // HTTP 上传
        "http": {
          "url": "http://panda-wiki-api:8000/api/v1/file/upload/anydoc"
        },
        "dir": "/kb_id"
      }
    }
    ↓
[7] Crawler Service 处理文档
    ├─ 调用飞书 API 获取文档内容
    ├─ 解析文档结构（标题、段落、表格等）
    ├─ 识别文档中的图片引用
    ├─ 下载所有图片到本地临时目录
    ├─ 转换文档格式（飞书格式 → HTML/Markdown）
    └─ 替换图片引用为本地路径
    ↓
[8] Crawler Service 上传图片
    对每个图片：
    ├─ 读取图片文件
    ├─ 构造 multipart/form-data 请求
    ├─ POST http://panda-wiki-api:8000/api/v1/file/upload/anydoc
    │   FormData:
    │   - file: 图片二进制数据
    │   - path: /kb_id/images/xxx.png
    ├─ API 验证请求来源（IP 白名单）
    ├─ API 上传图片到 MinIO/S3
    ├─ 返回图片 URL: /static-file/kb_id/images/xxx.png
    └─ Crawler 替换文档中的图片引用
    ↓
[9] Crawler Service 返回处理结果
    {
      "success": true,
      "data": {
        "task_id": "xxx",
        "status": "completed",
        "content": "处理后的 HTML/Markdown 内容"
      }
    }
    ↓
[10] API 保存文档到数据库
     图片 URL 已经是 PandaWiki 的内部路径
    ↓
[11] 用户可以正常查看文档和图片
```

---

## 三、图片处理核心机制

### 3.1 图片上传接口

**文件**: `backend/handler/v1/file.go`

```go
// UploadAnydoc - 专门用于 Crawler Service 上传图片
func (h *FileHandler) UploadAnydoc(c echo.Context) error {
    // 1. 安全验证：只允许 Crawler Service 调用
    clientIP := fmt.Sprintf("%s.17", h.config.SubnetPrefix)
    if utils.GetClientIPFromRemoteAddr(c) != clientIP {
        return c.JSON(http.StatusUnauthorized, domain.AnydocUploadResp{
            Code: 1,
            Err:  "invalid required",
        })
    }

    // 2. 接收图片文件
    file, err := c.FormFile("file")
    if err != nil {
        return c.JSON(http.StatusBadRequest, domain.AnydocUploadResp{
            Code: 1,
            Err:  "invalid required",
        })
    }

    // 3. 接收存储路径
    path := c.FormValue("path")
    if path == "" {
        return c.JSON(http.StatusBadRequest, domain.AnydocUploadResp{
            Code: 1,
            Err:  "invalid required",
        })
    }

    // 4. 上传到对象存储
    _, err = h.fileUsecase.AnyDocUploadFile(c.Request().Context(), file, path)
    if err != nil {
        return h.NewResponseWithError(c, "upload failed", err)
    }

    // 5. 返回图片访问 URL
    url := fmt.Sprintf("/static-file/%s", strings.TrimPrefix(path, "/"))
    
    return c.JSON(http.StatusOK, domain.AnydocUploadResp{
        Code: 0,
        Data: url,  // 返回给 Crawler Service
    })
}
```

### 3.2 Crawler Service 配置

**文件**: `backend/pkg/anydoc/anydoc.go`

```go
const (
    // 图片上传的目标 API
    apiUploaderUrl = "http://panda-wiki-api:8000/api/v1/file/upload/anydoc"
    
    // Crawler Service 地址
    crawlerServiceHost = "http://panda-wiki-crawler:8080"
    
    // 图片存储目录
    uploaderDir = "/image"
)

// HTTP 上传类型
const (
    uploaderTypeDefault UploaderType = iota
    uploaderTypeHTTP                          // 使用 HTTP 上传
)
```

### 3.3 飞书文档导出请求

**文件**: `backend/pkg/anydoc/feishu.go`

```go
func (c *Client) FeishuExportDoc(ctx context.Context, uuid, docID, fileType, spaceId, kbId string) (*UrlExportRes, error) {
    u, err := url.Parse(crawlerServiceHost)
    if err != nil {
        return nil, err
    }
    u.Path = feishuExportPath
    requestURL := u.String()

    // 构造请求体
    bodyMap := map[string]interface{}{
        "uuid":      uuid,
        "doc_id":    docID,
        "file_type": fileType,
        "space_id":  spaceId,
        "uploader": map[string]interface{}{
            "type": uploaderTypeHTTP,  // 使用 HTTP 上传
            "http": map[string]interface{}{
                "url": apiUploaderUrl,  // 图片上传目标地址
            },
            "dir": fmt.Sprintf("/%s", kbId),  // 存储目录：/kb_id
        },
    }

    // 发送请求到 Crawler Service
    jsonData, err := json.Marshal(bodyMap)
    req, err := http.NewRequestWithContext(ctx, http.MethodPost, requestURL, bytes.NewBuffer(jsonData))
    req.Header.Set("Content-Type", "application/json")

    resp, err := c.httpClient.Do(req)
    // ... 处理响应
}
```

---

## 四、图片导入成功的保障机制

### 4.1 多层保障机制

#### 1️⃣ **IP 白名单验证**

```go
// 只允许 Crawler Service 的 IP 调用上传接口
clientIP := fmt.Sprintf("%s.17", h.config.SubnetPrefix)
if utils.GetClientIPFromRemoteAddr(c) != clientIP {
    return c.JSON(http.StatusUnauthorized, ...)
}
```

**作用**：
- ✅ 防止外部恶意上传
- ✅ 确保只有 Crawler Service 可以上传图片
- ✅ 安全隔离

#### 2️⃣ **同步上传机制**

Crawler Service 在处理文档时：
```python
# 伪代码示例
for image in document.images:
    # 1. 下载图片
    image_data = download_from_feishu(image.url, access_token)
    
    # 2. 立即上传到 PandaWiki
    response = upload_to_pandawiki(image_data, path)
    
    # 3. 获取新的 URL
    new_url = response['data']
    
    # 4. 替换文档中的引用
    document.replace_image_url(image.url, new_url)
```

**作用**：
- ✅ 图片处理完成后才返回文档
- ✅ 确保所有图片都已上传
- ✅ 避免图片丢失

#### 3️⃣ **路径规范化**

```go
// 图片存储路径结构
/kb_id/images/uuid_filename.ext

// 示例
/01234567-89ab-cdef/images/a1b2c3d4_screenshot.png
```

**作用**：
- ✅ 按知识库隔离存储
- ✅ 避免文件名冲突
- ✅ 便于管理和清理

#### 4️⃣ **错误处理和重试**

```go
// Crawler Service 内部逻辑（推测）
func uploadImageWithRetry(imageData []byte, path string, maxRetries int) (string, error) {
    var lastErr error
    for i := 0; i < maxRetries; i++ {
        url, err := uploadImage(imageData, path)
        if err == nil {
            return url, nil
        }
        lastErr = err
        time.Sleep(time.Second * time.Duration(i+1))  // 指数退避
    }
    return "", fmt.Errorf("upload failed after %d retries: %w", maxRetries, lastErr)
}
```

**作用**：
- ✅ 网络波动时自动重试
- ✅ 提高上传成功率
- ✅ 记录失败原因

#### 5️⃣ **对象存储的可靠性**

使用 MinIO/S3 存储：
- ✅ 高可用性（多副本）
- ✅ 数据持久化
- ✅ 支持大规模存储

#### 6️⃣ **异步任务监控**

```go
// 通过 NATS 消息队列监控任务状态
func (c *Client) TaskWaitForCompletion(ctx context.Context, taskID string) (*domain.AnydocTaskExportEvent, error) {
    // 订阅任务完成事件
    taskChan := make(chan *domain.AnydocTaskExportEvent, 1)
    c.taskWaiters[taskID] = taskChan

    // 等待任务完成或超时
    select {
    case event := <-taskChan:
        return event, nil
    case <-ctx.Done():
        return nil, ctx.Err()
    }
}
```

**作用**：
- ✅ 实时监控导入进度
- ✅ 及时发现失败任务
- ✅ 支持超时控制

---

## 五、图片处理详细流程

### 5.1 飞书图片的特殊性

飞书文档中的图片有以下特点：

1. **需要认证访问**
   - 图片 URL 需要 access_token
   - 有时效性限制
   - 不能直接引用

2. **多种图片类型**
   - 内联图片（inline image）
   - 附件图片（attachment）
   - 表格中的图片
   - 公式渲染的图片

3. **格式多样**
   - PNG、JPEG、GIF、WebP
   - SVG（公式、图表）
   - 可能需要格式转换

### 5.2 Crawler Service 的图片处理

```python
# 伪代码：Crawler Service 的图片处理逻辑

class FeishuDocumentProcessor:
    def export_document(self, doc_id, uploader_config):
        # 1. 获取文档内容
        doc_content = self.feishu_api.get_document(doc_id, self.access_token)
        
        # 2. 解析文档，提取图片引用
        images = self.extract_images(doc_content)
        
        # 3. 下载并上传所有图片
        image_url_mapping = {}
        for image in images:
            try:
                # 3.1 从飞书下载图片
                image_data = self.download_image_from_feishu(
                    image.url, 
                    self.access_token
                )
                
                # 3.2 生成存储路径
                filename = f"{uuid.uuid4()}_{image.name}"
                path = f"{uploader_config['dir']}/images/{filename}"
                
                # 3.3 上传到 PandaWiki
                new_url = self.upload_to_pandawiki(
                    image_data,
                    path,
                    uploader_config['http']['url']
                )
                
                # 3.4 记录映射关系
                image_url_mapping[image.url] = new_url
                
            except Exception as e:
                self.logger.error(f"Failed to process image {image.url}: {e}")
                # 可以选择：跳过该图片 或 使用占位符
                image_url_mapping[image.url] = "/static/placeholder.png"
        
        # 4. 替换文档中的图片引用
        processed_content = self.replace_image_urls(doc_content, image_url_mapping)
        
        # 5. 转换文档格式
        html_content = self.convert_to_html(processed_content)
        
        # 6. 返回处理结果
        return {
            "success": True,
            "content": html_content,
            "images_count": len(images),
            "images_success": len([url for url in image_url_mapping.values() if url != "/static/placeholder.png"])
        }
    
    def download_image_from_feishu(self, image_url, access_token):
        """从飞书下载图片"""
        headers = {
            "Authorization": f"Bearer {access_token}"
        }
        response = requests.get(image_url, headers=headers, timeout=30)
        response.raise_for_status()
        return response.content
    
    def upload_to_pandawiki(self, image_data, path, upload_url):
        """上传图片到 PandaWiki"""
        files = {
            'file': ('image.png', image_data, 'image/png')
        }
        data = {
            'path': path
        }
        response = requests.post(upload_url, files=files, data=data, timeout=30)
        response.raise_for_status()
        result = response.json()
        if result['code'] == 0:
            return result['data']  # 返回新的 URL
        else:
            raise Exception(f"Upload failed: {result['err']}")
```

### 5.3 图片 URL 转换示例

**原始飞书图片 URL**：
```
https://open.feishu.cn/open-apis/drive/v1/medias/xxx/download?access_token=yyy
```

**下载后上传到 PandaWiki**：
```
POST http://panda-wiki-api:8000/api/v1/file/upload/anydoc
FormData:
  - file: [图片二进制数据]
  - path: /kb_123456/images/a1b2c3d4_screenshot.png
```

**返回的新 URL**：
```
/static-file/kb_123456/images/a1b2c3d4_screenshot.png
```

**文档中的最终引用**：
```html
<img src="/static-file/kb_123456/images/a1b2c3d4_screenshot.png" alt="截图">
```

---

## 六、可能的失败场景及处理

### 6.1 常见失败场景

| 场景 | 原因 | 影响 | 处理方式 |
|------|------|------|----------|
| **飞书 Token 过期** | access_token 失效 | 无法下载图片 | 提示用户重新授权 |
| **网络超时** | 飞书 API 响应慢 | 部分图片下载失败 | 重试机制 + 超时设置 |
| **图片过大** | 单个图片超过限制 | 上传失败 | 压缩图片或跳过 |
| **存储空间不足** | MinIO/S3 空间满 | 无法保存图片 | 返回错误，提示扩容 |
| **格式不支持** | 特殊图片格式 | 无法处理 | 转换格式或使用占位符 |
| **权限问题** | 无权访问某些图片 | 下载失败 | 使用占位符 |

### 6.2 错误处理策略

#### 策略一：全部成功或全部失败

```python
def export_document_strict(self, doc_id):
    try:
        images = self.extract_images(doc_content)
        for image in images:
            # 任何一个图片失败都抛出异常
            self.process_image(image)
        return success_response
    except Exception as e:
        # 回滚已上传的图片
        self.rollback_uploaded_images()
        return error_response
```

**优点**：数据一致性好  
**缺点**：一个图片失败导致整个文档导入失败

#### 策略二：尽力而为（推荐）

```python
def export_document_best_effort(self, doc_id):
    images = self.extract_images(doc_content)
    success_count = 0
    failed_images = []
    
    for image in images:
        try:
            self.process_image(image)
            success_count += 1
        except Exception as e:
            failed_images.append({
                "url": image.url,
                "error": str(e)
            })
            # 使用占位符
            image_url_mapping[image.url] = "/static/placeholder.png"
    
    return {
        "success": True,
        "content": processed_content,
        "images_total": len(images),
        "images_success": success_count,
        "images_failed": failed_images
    }
```

**优点**：用户体验好，部分失败不影响整体  
**缺点**：可能有图片显示为占位符

---

## 七、性能优化

### 7.1 并发下载和上传

```python
import asyncio
import aiohttp

async def process_images_concurrent(self, images, max_concurrent=5):
    """并发处理图片，限制并发数"""
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_one_image(image):
        async with semaphore:
            try:
                # 下载
                image_data = await self.download_image_async(image.url)
                # 上传
                new_url = await self.upload_image_async(image_data, image.path)
                return (image.url, new_url)
            except Exception as e:
                return (image.url, None)
    
    # 并发处理所有图片
    tasks = [process_one_image(img) for img in images]
    results = await asyncio.gather(*tasks)
    
    return dict(results)
```

**效果**：
- ✅ 大幅减少总处理时间
- ✅ 充分利用网络带宽
- ✅ 控制并发数避免过载

### 7.2 图片缓存

```python
class ImageCache:
    def __init__(self):
        self.cache = {}  # {feishu_url: pandawiki_url}
    
    def get_or_upload(self, feishu_url, upload_func):
        """如果图片已经上传过，直接返回缓存的 URL"""
        if feishu_url in self.cache:
            return self.cache[feishu_url]
        
        new_url = upload_func()
        self.cache[feishu_url] = new_url
        return new_url
```

**适用场景**：
- 同一个图片在多个文档中使用
- 批量导入时避免重复上传

### 7.3 图片压缩

```python
from PIL import Image
import io

def compress_image(image_data, max_size_mb=5, quality=85):
    """压缩图片到指定大小"""
    img = Image.open(io.BytesIO(image_data))
    
    # 如果图片小于限制，直接返回
    if len(image_data) < max_size_mb * 1024 * 1024:
        return image_data
    
    # 压缩
    output = io.BytesIO()
    img.save(output, format='JPEG', quality=quality, optimize=True)
    compressed_data = output.getvalue()
    
    return compressed_data
```

---

## 八、监控和日志

### 8.1 关键日志点

```go
// API 端
h.logger.Debug("AnydocUpload file", "path", path)
h.logger.Debug("AnydocUpload file", "url", url)

// Crawler 端
c.logger.Info("FeishuListDocs", "requestURL:", requestURL, "resp", string(respBody))
c.logger.Info("FeishuDoc", "requestURL:", requestURL, "body", string(jsonData), "resp", string(respBody))
```

### 8.2 监控指标

建议监控以下指标：

| 指标 | 说明 | 告警阈值 |
|------|------|----------|
| **图片上传成功率** | 成功上传数 / 总图片数 | < 95% |
| **平均上传时间** | 单张图片上传耗时 | > 5s |
| **存储空间使用率** | MinIO/S3 使用率 | > 80% |
| **Crawler Service 响应时间** | 文档导出耗时 | > 30s |
| **失败任务数** | 导入失败的文档数 | > 5/小时 |

---

## 九、最佳实践建议

### 9.1 用户侧

1. **确保 Token 有效**
   - 使用前检查 access_token 是否过期
   - 建议使用长期有效的 token

2. **分批导入**
   - 大量文档分批导入
   - 避免一次性导入过多文档

3. **检查导入结果**
   - 导入后检查图片是否正常显示
   - 查看导入日志了解失败原因

### 9.2 系统侧

1. **配置合理的超时时间**
   ```go
   httpClient: &http.Client{
       Timeout: 60 * time.Second,  // 根据实际情况调整
   }
   ```

2. **设置图片大小限制**
   ```go
   const MaxImageSize = 10 * 1024 * 1024  // 10MB
   ```

3. **定期清理临时文件**
   - Crawler Service 的临时目录
   - 失败任务的残留文件

4. **监控存储空间**
   - 设置存储空间告警
   - 定期清理无用图片

---

## 十、总结

### 图片导入成功的核心保障

1. ✅ **同步处理机制**
   - 图片全部上传完成后才返回文档
   - 确保文档和图片的一致性

2. ✅ **可靠的存储**
   - 使用 MinIO/S3 对象存储
   - 数据持久化和高可用

3. ✅ **完善的错误处理**
   - 重试机制
   - 降级策略（占位符）
   - 详细的错误日志

4. ✅ **安全的上传通道**
   - IP 白名单验证
   - 专用上传接口
   - 路径规范化

5. ✅ **异步任务监控**
   - NATS 消息队列
   - 实时状态更新
   - 超时控制

### 关键技术点

| 技术点 | 作用 | 重要性 |
|--------|------|--------|
| **Crawler Service** | 文档解析和图片处理 | ⭐⭐⭐⭐⭐ |
| **HTTP 上传接口** | 图片传输通道 | ⭐⭐⭐⭐⭐ |
| **对象存储** | 图片持久化 | ⭐⭐⭐⭐⭐ |
| **IP 白名单** | 安全保障 | ⭐⭐⭐⭐ |
| **消息队列** | 异步任务监控 | ⭐⭐⭐ |
| **并发处理** | 性能优化 | ⭐⭐⭐ |

### 回答你的问题

> **飞书导入文档的时候，图片是如何确保导入成功的？**

**答案**：

PandaWiki 通过以下机制确保图片导入成功：

1. **Crawler Service 同步处理**：在文档导出过程中，先下载所有图片并上传到 PandaWiki，替换图片引用后才返回文档内容。

2. **专用上传接口**：提供 `/api/v1/file/upload/anydoc` 接口，专门用于 Crawler Service 上传图片，通过 IP 白名单保证安全。

3. **可靠存储**：图片上传到 MinIO/S3 对象存储，保证数据持久化和高可用。

4. **错误处理**：支持重试机制，失败时可以使用占位符，不影响整体导入。

5. **路径规范化**：按知识库隔离存储（`/kb_id/images/xxx.png`），避免冲突。

6. **异步监控**：通过 NATS 消息队列监控任务状态，及时发现和处理失败。

**核心原理**：图片不是直接引用飞书的 URL，而是先下载到 Crawler Service，再上传到 PandaWiki 的对象存储，最后替换为 PandaWiki 的内部 URL。这样确保了图片的可访问性和持久性。

---

**文档创建时间**：2025-11-25  
**分析工具**：Kiro AI Assistant
